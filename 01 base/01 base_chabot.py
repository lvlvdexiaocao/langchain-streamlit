
import streamlit as st
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.messages import HumanMessage, AIMessage
import os
# å…³é—­LangSmithè¿½è¸ª
os.environ["LANGCHAIN_TRACING_V2"] = "false"


model = ChatOllama(model="qwen2.5:3b")

st.set_page_config(page_title="èŠå¤©æœºå™¨äºº",page_icon="ğŸ¤–")

st.title("Base_ChatBot")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

def get_response(user_query, chat_history):
    template = """
    ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚è¯·ç»“åˆå¯¹è¯å†å²å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
    èŠå¤©å†å²ï¼š{chat_history}
    ç”¨æˆ·é—®é¢˜ï¼š{user_question}
    """
    prompt = ChatPromptTemplate.from_template(template)
    chain = prompt|model|StrOutputParser()

    return chain.stream({
        "chat_history": chat_history,
        "user_question": user_query,
    })

for message in st.session_state.chat_history:
    if isinstance(message, HumanMessage):
        with st.chat_message("Human"):
            st.markdown(message.content)
    else:
        with st.chat_message("AI"):
            st.markdown(message.content)

user_query = st.chat_input("è¯·è¾“å…¥")
if user_query is not None and user_query !="":
    st.session_state.chat_history.append(HumanMessage(user_query))

    with st.chat_message("Human"):
        st.markdown(user_query)

    with st.chat_message("AI"):
        ai_response = st.write_stream(get_response(user_query, st.session_state.chat_history))


    st.session_state.chat_history.append(AIMessage(ai_response))